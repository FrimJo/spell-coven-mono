# Data Model: CLIP Model Alignment & Pipeline Optimization

**Feature**: 012-fix-clip-model-alignment  
**Date**: 2025-10-17

## Overview

This feature updates the browser's CLIP embedding pipeline to align with the Python pipeline. The primary data model change is the embedding dimension: **512 → 768**. All other data structures remain compatible.

---

## Entities

### CLIPModelConfig

**Purpose**: Configuration for CLIP model loading and preprocessing

**Fields**:
- `modelId: string` - Hugging Face model identifier (e.g., `"Xenova/clip-vit-large-patch14-336"`)
- `embeddingDim: number` - Expected embedding dimension (768)
- `inputSize: number` - Expected input image size (336)
- `dtype: string` - Model precision (`"fp32"` for ViT-L/14)
- `device: string` - Compute device (`"auto"` for WebGPU/WebGL/WASM fallback)

**Validation Rules**:
- `modelId` must be non-empty string
- `embeddingDim` must be 768 (matches database format)
- `inputSize` must be 336 (matches ViT-L/14@336px native input)
- `dtype` must be `"fp16"` or `"fp32"`
- `device` must be `"auto"`, `"webgpu"`, `"webgl"`, or `"wasm"`

**State Transitions**: Immutable configuration (no state changes)

**Relationships**:
- Used by `CLIPEmbedder` for model initialization
- Referenced in validation error messages

---

### QueryEmbedding

**Purpose**: Represents a card embedding generated from user click

**Fields**:
- `vector: Float32Array` - **768-dimensional** L2-normalized embedding (CHANGED from 512)
- `isNormalized: boolean` - Whether vector is L2-normalized (should always be true)
- `norm: number` - L2 norm of vector (should be ~1.0 ±0.008)

**Validation Rules**:
- `vector.length` must equal 768 (CHANGED from 512)
- `isNormalized` must be true
- `norm` must be within [0.992, 1.008] (1.0 ±0.008 tolerance)
- No NaN or Infinity values in vector

**State Transitions**: Immutable after creation (embeddings don't change)

**Relationships**:
- Generated by `CLIPEmbedder.embedFromCanvas()`
- Consumed by similarity search (dot product with database embeddings)
- Validated before search execution

---

### DatabaseEmbeddings

**Purpose**: Pre-computed card embeddings loaded from server

**Fields**:
- `embeddings: Int8Array` - **768-dimensional** quantized embeddings, shape [N, 768] (CHANGED from [N, 512])
- `metadata: EmbeddingMetadata` - Metadata describing embeddings format
- `cards: CardRecord[]` - Card information for each embedding

**Validation Rules**:
- `embeddings.length` must equal `N * 768` where N is number of cards
- `metadata.shape[1]` must equal 768 (CHANGED from 512)
- `metadata.records.length` must equal N
- `cards.length` must equal N

**State Transitions**: Loaded once on app initialization, immutable thereafter

**Relationships**:
- Loaded by `EmbeddingsLoader`
- Validated by `ContractValidator`
- Used by similarity search for finding matching cards

---

### EmbeddingMetadata

**Purpose**: Describes the format and version of embedding data

**Fields**:
- `version: string` - Data format version (e.g., `"0.3.0"`)
- `shape: [number, number]` - Embedding array shape [N, 768] (CHANGED from [N, 512])
- `model: string` - Model used to generate embeddings (e.g., `"ViT-L/14@336px"`)
- `quantization: string` - Quantization method (`"int8"`)
- `records: CardRecord[]` - Card metadata for each embedding

**Validation Rules**:
- `version` must match expected format (semantic versioning)
- `shape[0]` must equal number of cards
- `shape[1]` must equal 768 (CHANGED from 512)
- `model` must be `"ViT-L/14@336px"` (CHANGED from `"ViT-B/32"`)
- `quantization` must be `"int8"`
- `records` must be non-empty array

**State Transitions**: Immutable after loading

**Relationships**:
- Part of `DatabaseEmbeddings`
- Validated by `ContractValidator`
- Used in error messages for dimension mismatches

---

### ModelLoadingState

**Purpose**: Tracks CLIP model loading progress

**Fields**:
- `status: "not-loaded" | "loading" | "ready" | "error"` - Current loading state
- `progress?: string` - Progress message (e.g., `"Downloading: model.onnx - 45%"`)
- `error?: string` - Error message if loading failed
- `retryCount: number` - Number of retry attempts (max 3)

**Validation Rules**:
- `status` must be one of the four valid states
- `progress` only present when `status === "loading"`
- `error` only present when `status === "error"`
- `retryCount` must be in range [0, 3]

**State Transitions**:
```
not-loaded → loading → ready (success)
not-loaded → loading → error (failure, retryCount < 3)
error → loading (retry)
error → error (retryCount === 3, permanent failure)
```

**Relationships**:
- Managed by `CLIPEmbedder`
- Exposed to React components via hooks
- Determines UI loading indicators and error banners

---

### PreprocessedCanvas

**Purpose**: Represents a canvas at the correct size for CLIP input

**Fields**:
- `canvas: HTMLCanvasElement` - Canvas element containing preprocessed image
- `width: number` - Canvas width (336) (CHANGED from 384)
- `height: number` - Canvas height (336) (CHANGED from 384)
- `sourceWidth: number` - Original source width (e.g., 384 from SlimSAM)
- `sourceHeight: number` - Original source height (e.g., 384 from SlimSAM)

**Validation Rules**:
- `width` must equal 336 (CHANGED from 384)
- `height` must equal 336 (CHANGED from 384)
- `canvas.width` must equal `width`
- `canvas.height` must equal `height`
- Canvas must not be empty (has pixel data)

**State Transitions**: Immutable after creation (canvas content doesn't change)

**Relationships**:
- Created from SlimSAM warped canvas (384×384)
- Consumed by `CLIPEmbedder.embedFromCanvas()`
- Preprocessing handled automatically by Transformers.js

---

## Data Flow

### Card Click to Search Result

```
1. User clicks card in video stream
   ↓
2. SlimSAM segments and warps card → 384×384 canvas
   ↓
3. [REMOVED] Resize to 446×620 (unnecessary intermediate step)
   ↓
4. CLIPEmbedder.embedFromCanvas(384×384 canvas)
   ├─ Transformers.js automatic preprocessing: 384×384 → 336×336 with black padding
   ├─ CLIP ViT-L/14@336px inference
   └─ Output: 768-dim Float32Array embedding
   ↓
5. Validate embedding (dimension, normalization)
   ↓
6. Similarity search: dot product with database embeddings (int8 → float32 conversion)
   ↓
7. Return top-K results with scores
```

### Model Loading Flow

```
1. Page loads → Model NOT loaded (lazy loading)
   ↓
2. User clicks first card
   ↓
3. CLIPEmbedder.initialize() called
   ├─ Status: not-loaded → loading
   ├─ Download model from Hugging Face CDN
   ├─ Show progress indicator in UI
   └─ Cache in IndexedDB
   ↓
4a. Success: Status → ready, proceed with embedding
4b. Failure: Status → error, retry (up to 3 times)
4c. After 3 failures: Permanent error, show banner
   ↓
5. Subsequent clicks use cached model (no re-download)
```

---

## Breaking Changes

### Embedding Dimension: 512 → 768

**Impact**: All browser-generated embeddings are now 768-dimensional instead of 512-dimensional.

**Migration**:
1. Database embeddings must be regenerated with Python pipeline using ViT-L/14@336px
2. Old 512-dim embeddings are incompatible and will cause dimension mismatch errors
3. Browser validation will detect mismatch and show clear error message

**Error Message**:
```
Query dimension mismatch: expected 768, got 512.
Ensure browser uses ViT-L/14@336px model.
Database may need regeneration with updated Python pipeline.
```

### Canvas Dimensions: 384 → 336

**Impact**: CLIP input size changed from 384×384 to 336×336 (matches model native input).

**Migration**:
1. Update `CROPPED_CARD_WIDTH` and `CROPPED_CARD_HEIGHT` constants to 336
2. Remove any hardcoded 384 references in CLIP preprocessing
3. SlimSAM output remains 384×384 (no changes needed)

**Note**: This is handled automatically by Transformers.js preprocessing. No manual resize needed.

---

## Validation Rules Summary

### At Embedding Generation
- Embedding dimension must be 768
- L2 norm must be ~1.0 ±0.008
- No NaN or Infinity values

### At Database Loading
- Metadata shape[1] must be 768
- Binary file size must equal N * 768 bytes
- Model must be "ViT-L/14@336px"

### At Search Execution
- Query dimension must match database dimension (768)
- Query must be L2-normalized
- Database must be loaded and non-empty

---

## Backward Compatibility

**None** - This is a breaking change. Old 512-dimensional embeddings are incompatible with the new 768-dimensional format. Users must regenerate their embedding databases using the updated Python pipeline.

**Detection**: Dimension mismatch errors will occur immediately when attempting to search with mismatched dimensions. Error messages clearly explain the issue and provide migration instructions.

**Migration Path**:
1. Run Python pipeline with ViT-L/14@336px model (already default in `packages/mtg-image-db`)
2. Export embeddings for browser using `export_for_browser.py`
3. Deploy updated embeddings (768-dim int8 format)
4. Browser automatically validates and uses new format
